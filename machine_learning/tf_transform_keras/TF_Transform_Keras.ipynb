{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab: TF Transform # \n",
    "\n",
    "**Learning Objectives**\n",
    "1. Preproccess data and engineer new features using TF Transform \n",
    "1. Create and deploy Apache Beam pipeline \n",
    "1. Use processed data to train model locally then serve a prediction\n",
    "\n",
    "## Introduction \n",
    "While Pandas is fine for experimenting, for operationalization of your workflow it is better to do preprocessing in Apache Beam. This will also help if you need to preprocess data in flight, since Apache Beam allows for streaming. In this lab we will pull data from BigQuery then use Apache Beam and TF Transform to process the data.  \n",
    "\n",
    "Only specific combinations of TensorFlow/Beam are supported by tf.transform so make sure to get a combo that works. In this lab we will be using: \n",
    "* TFT 0.24.0\n",
    "* TF 2.3.0 \n",
    "* Apache Beam [GCP] 2.24.0\n",
    "\n",
    "## Setup ##\n",
    "\n",
    "Before starting, we need to ensure the proper versions of packages are installed. \n",
    "\n",
    "**After running the following cell be sure to restart the kernel!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install tensorflow==2.3.0 tensorflow-transform==0.24.0 apache-beam[gcp]==2.24.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we will import the packages we need for the notebook. Note that we will be using the implementation of Apache Beam in TensorFlow Transform for this lab. Ignore any errors referring to `tfx_bsl` for this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import math\n",
    "import os\n",
    "import pprint\n",
    "import tempfile\n",
    "\n",
    "from absl import logging\n",
    "import apache_beam as beam\n",
    "import tensorflow as tf\n",
    "import tensorflow_transform as tft\n",
    "import tensorflow_transform.beam as tft_beam\n",
    "from tensorflow_transform.tf_metadata import dataset_metadata\n",
    "from tensorflow_transform.tf_metadata import schema_utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we will download our data and place it into a folder. Also we will create a directory for our metadata from the TF Transform job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "# Remove files from previous notebook runs if necessary\n",
    "rm -r input_data\n",
    "rm -r working_dir\n",
    "\n",
    "mkdir ./input_data\n",
    "mkdir ./working_dir\n",
    "\n",
    "wget https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data \n",
    "wget https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.test\n",
    "    \n",
    "mv adult.* ./input_data/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will define lists with the CSV column names and split the columns based on if they are refer to categorical or numeric features. Note that `education-num` is an optional feature (i.e. it can have a value of `None`), so we will put it in a different list as we will need to treat it differerently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ORDERED_CSV_COLUMNS = [\n",
    "    'age', 'workclass', 'fnlwgt', 'education', 'education-num',\n",
    "    'marital-status', 'occupation', 'relationship', 'race', 'sex',\n",
    "    'capital-gain', 'capital-loss', 'hours-per-week', 'native-country', 'label'\n",
    "]\n",
    "\n",
    "CATEGORICAL_FEATURE_KEYS = [\n",
    "    'workclass',\n",
    "    'education',\n",
    "    'marital-status',\n",
    "    'occupation',\n",
    "    'relationship',\n",
    "    'race',\n",
    "    'sex',\n",
    "    'native-country',\n",
    "]\n",
    "NUMERIC_FEATURE_KEYS = [\n",
    "    'age',\n",
    "    'capital-gain',\n",
    "    'capital-loss',\n",
    "    'hours-per-week',\n",
    "]\n",
    "OPTIONAL_NUMERIC_FEATURE_KEYS = [\n",
    "    'education-num',\n",
    "]\n",
    "LABEL_KEY = 'label'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing pipeline in TF Transform ##\n",
    "\n",
    "Now we're ready to create our pipeline. In a bit we will be creating a preprocessing function (`preprocessing_fn`) to apply our `tf.transform` operations to our data, then we will create an Apache Beam pipeline to carry out the operations and collect metadata to repeat the transformations at serving time. \n",
    "\n",
    "First though, let us create a custom `PTransform` that will allow us to filter out errors when applying `beam.Map`-like operations. This keep the pipeline from crashing because of a single bad line of data, while letting us keep track of the number of bad rows. If this number is a large percentage of our number of instances, then we would want to more carefully explore our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MapAndFilterErrors(beam.PTransform):\n",
    "  \"\"\"Like beam.Map but filters out errors in the map_fn.\"\"\"\n",
    "\n",
    "  class _MapAndFilterErrorsDoFn(beam.DoFn):\n",
    "    \"\"\"Count the bad examples using a beam metric.\"\"\"\n",
    "\n",
    "    def __init__(self, fn):\n",
    "      self._fn = fn\n",
    "      # Create a counter to measure number of bad elements.\n",
    "      self._bad_elements_counter = beam.metrics.Metrics.counter(\n",
    "          'census_example', 'bad_elements')\n",
    "\n",
    "    def process(self, element):\n",
    "      try:\n",
    "        yield self._fn(element)\n",
    "      except Exception:  # pylint: disable=broad-except\n",
    "        # Catch any exception the above call.\n",
    "        self._bad_elements_counter.inc(1)\n",
    "\n",
    "  def __init__(self, fn):\n",
    "    self._fn = fn\n",
    "\n",
    "  def expand(self, pcoll):\n",
    "    return pcoll | beam.ParDo(self._MapAndFilterErrorsDoFn(self._fn))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we need to define a `feature_spec` to inform our TF Transform pipeline how to expect incoming data, that is to define a schema. This information will be saved as Dataset metadata and shared with the model at serving time as well. We will go ahead and define some constants that we will need later in the process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RAW_DATA_FEATURE_SPEC = dict([(name, tf.io.FixedLenFeature([], tf.string))\n",
    "                              for name in CATEGORICAL_FEATURE_KEYS] +\n",
    "                             [(name, tf.io.FixedLenFeature([], tf.float32))\n",
    "                              for name in NUMERIC_FEATURE_KEYS] +\n",
    "                             [(name, tf.io.VarLenFeature(tf.float32))  \n",
    "                              for name in OPTIONAL_NUMERIC_FEATURE_KEYS] +\n",
    "                             [(LABEL_KEY,\n",
    "                               tf.io.FixedLenFeature([], tf.string))])\n",
    "\n",
    "RAW_DATA_METADATA = dataset_metadata.DatasetMetadata(\n",
    "    schema_utils.schema_from_feature_spec(RAW_DATA_FEATURE_SPEC))\n",
    "\n",
    "# Constants used for training.  Note that the number of instances will be\n",
    "# computed by tf.Transform in future versions, in which case it can be read from\n",
    "# the metadata. \n",
    "TRAIN_BATCH_SIZE = 128\n",
    "TRAIN_NUM_EPOCHS = 50\n",
    "NUM_TRAIN_INSTANCES = 32561\n",
    "NUM_TEST_INSTANCES = 16281\n",
    "\n",
    "# Names of temp files\n",
    "TRANSFORMED_TRAIN_DATA_FILEBASE = 'train_transformed'\n",
    "TRANSFORMED_TEST_DATA_FILEBASE = 'test_transformed'\n",
    "EXPORTED_MODEL_DIR = 'exported_model_dir'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we define our Apache Beam pipeline, we will create our preprocessing function. We will do the following:\n",
    "\n",
    "1. Scale all numeric features to the range \\[0,1\\] using `tft.scale_to_0_1`\n",
    "2. We will pack our optional numeric feature into a sparse tensor (since we could have null values) and then convert to a dense tensor. After that we will need to drop a dimension before scaling the feature as we did above using `tft.scale_to_0_1`\n",
    "3. For the categorical features, we will use `tft.compute_and_apply_vocabulary` to run through the data and compile a dictionary of feature values and then return a `Tensor` with the corresponding indexes in the vocabular list for each instance.\n",
    "4. Then we will replace the original labels with integer valued labels (0 or 1).\n",
    "5. Finally, we will remove trailing periods when the data is read with `tf.data`. This is of course data specific, and you would need to explore the data to see that this is necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing_fn(inputs):\n",
    "    \"\"\"Preprocess input columns into transformed columns.\"\"\"\n",
    "    # Since we are modifying some features and leaving others unchanged, we\n",
    "    # start by setting `outputs` to a copy of `inputs.\n",
    "    outputs = inputs.copy()\n",
    "\n",
    "    # Scale numeric columns to have range [0, 1].\n",
    "    for key in NUMERIC_FEATURE_KEYS:\n",
    "        outputs[key] = tft.scale_to_0_1(inputs[key])\n",
    "\n",
    "    for key in OPTIONAL_NUMERIC_FEATURE_KEYS:\n",
    "        # This is a SparseTensor because it is optional. Here we fill in a default\n",
    "        # value when it is missing.\n",
    "        sparse = tf.sparse.SparseTensor(inputs[key].indices, inputs[key].values,\n",
    "                                        [inputs[key].dense_shape[0], 1])\n",
    "        dense = tf.sparse.to_dense(sp_input=sparse, default_value=0.)\n",
    "        # Reshaping from a batch of vectors of size 1 to a batch to scalars.\n",
    "        dense = tf.squeeze(dense, axis=1)\n",
    "        outputs[key] = tft.scale_to_0_1(dense)\n",
    "\n",
    "    # For all categorical columns except the label column, we generate a\n",
    "    # vocabulary but do not modify the feature.  This vocabulary is instead\n",
    "    # used in the trainer, by means of a feature column, to convert the feature\n",
    "    # from a string to an integer id.\n",
    "    for key in CATEGORICAL_FEATURE_KEYS:\n",
    "        outputs[key] = tft.compute_and_apply_vocabulary(\n",
    "                           tf.strings.strip(inputs[key]), num_oov_buckets=1, vocab_filename=key)\n",
    "\n",
    "    # For the label column we provide the mapping from string to index.\n",
    "    table_keys = ['>50K', '<=50K']\n",
    "    initializer = tf.lookup.KeyValueTensorInitializer(\n",
    "        keys=table_keys,\n",
    "        values=tf.cast(tf.range(len(table_keys)), tf.int64),\n",
    "        key_dtype=tf.string,\n",
    "        value_dtype=tf.int64)\n",
    "        table = tf.lookup.StaticHashTable(initializer, default_value=-1)\n",
    "\n",
    "    # Romove trailing periods for test data when the data is read with tf.data.\n",
    "    label_str = tf.strings.regex_replace(inputs[LABEL_KEY], r'\\.', '')\n",
    "    label_str = tf.strings.strip(label_str)\n",
    "    data_labels = table.lookup(label_str)\n",
    "    transformed_label = tf.one_hot(\n",
    "    indices=data_labels, depth=len(table_keys), on_value=1.0, off_value=0.0)\n",
    "    outputs[LABEL_KEY] = tf.reshape(transformed_label, [-1, len(table_keys)])\n",
    "\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will define our Apache Beam pipeline. The `transform_data` function will be used to define and run our pipeline. We will apply a PTransform to parse our data and apply our custom PTransform defined earlier, after that we will apply our `preprocessing_fn` using `AnalayzeAndTransformDataset`. After that we will encode our transformed data into the TFRecord format, and then apply `TransformDataset` to transform our test dataset as well. Finally we will write out our `transform_fn` and metadata for the dataset so that we can also reference it at serving time.\n",
    "\n",
    "In this notebook we will run the Apache Beam pipeline locally since this is a smaller dataset. If we were using a larger dataset we could pass the proper options to the pipeline to run the pipeline on Dataflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_data(train_data_file, test_data_file, working_dir):\n",
    "  \"\"\"Transform the data and write out as a TFRecord of Example protos.\n",
    "\n",
    "  Read in the data using the CSV reader, and transform it using a\n",
    "  preprocessing pipeline that scales numeric data and converts categorical data\n",
    "  from strings to int64 values indices, by creating a vocabulary for each\n",
    "  category.\n",
    "\n",
    "  Args:\n",
    "    train_data_file: File containing training data\n",
    "    test_data_file: File containing test data\n",
    "    working_dir: Directory to write transformed data and metadata to\n",
    "  \"\"\"\n",
    "\n",
    "# The \"with\" block will create a pipeline, and run that pipeline at the exit\n",
    "  # of the block.\n",
    "  with beam.Pipeline() as pipeline:\n",
    "    with tft_beam.Context(temp_dir=tempfile.mkdtemp()):\n",
    "      # Create a coder to read the census data with the schema.  To do this we\n",
    "      # need to list all columns in order since the schema doesn't specify the\n",
    "      # order of columns in the csv.\n",
    "      converter = tft.coders.CsvCoder(ORDERED_CSV_COLUMNS,\n",
    "                                      RAW_DATA_METADATA.schema)\n",
    "\n",
    "      # Read in raw data and convert using CSV converter.  Note that we apply\n",
    "      # some Beam transformations here, which will not be encoded in the TF\n",
    "      # graph since we don't do the from within tf.Transform's methods\n",
    "      # (AnalyzeDataset, TransformDataset etc.).  These transformations are just\n",
    "      # to get data into a format that the CSV converter can read, in particular\n",
    "      # removing spaces after commas.\n",
    "      #\n",
    "      # We use MapAndFilterErrors instead of Map to filter out decode errors in\n",
    "      # convert.decode which should only occur for the trailing blank line.\n",
    "    \n",
    "      raw_data = (\n",
    "          pipeline\n",
    "          | 'ReadTrainData' >> beam.io.ReadFromText(train_data_file)\n",
    "          | 'FixCommasTrainData' >> beam.Map(\n",
    "              lambda line: line.replace(', ', ','))\n",
    "          | 'DecodeTrainData' >> MapAndFilterErrors(converter.decode))\n",
    "\n",
    "      # Combine data and schema into a dataset tuple.  Note that we already used\n",
    "      # the schema to read the CSV data, but we also need it to interpret\n",
    "      # raw_data.\n",
    "      raw_dataset = (raw_data, RAW_DATA_METADATA)\n",
    "        \n",
    "      transformed_dataset, transform_fn = (\n",
    "          raw_dataset | tft_beam.AnalyzeAndTransformDataset(preprocessing_fn))\n",
    "    \n",
    "      transformed_data, transformed_metadata = transformed_dataset\n",
    "        \n",
    "      transformed_data_coder = tft.coders.ExampleProtoCoder(\n",
    "          transformed_metadata.schema)\n",
    "\n",
    "      _ = (\n",
    "          transformed_data\n",
    "          | 'EncodeTrainData' >> beam.Map(transformed_data_coder.encode)\n",
    "          | 'WriteTrainData' >> beam.io.WriteToTFRecord(\n",
    "              os.path.join(working_dir, TRANSFORMED_TRAIN_DATA_FILEBASE)))\n",
    "\n",
    "      # Now apply transform function to test data.  In this case we remove the\n",
    "      # trailing period at the end of each line, and also ignore the header line\n",
    "      # that is present in the test data file.\n",
    "      raw_test_data = (\n",
    "          pipeline\n",
    "          | 'ReadTestData' >> beam.io.ReadFromText(test_data_file,\n",
    "                                                   skip_header_lines=1)\n",
    "          | 'FixCommasTestData' >> beam.Map(\n",
    "              lambda line: line.replace(', ', ','))\n",
    "          | 'RemoveTrailingPeriodsTestData' >> beam.Map(lambda line: line[:-1])\n",
    "          | 'DecodeTestData' >> MapAndFilterErrors(converter.decode))\n",
    "\n",
    "      raw_test_dataset = (raw_test_data, RAW_DATA_METADATA)\n",
    "\n",
    "      transformed_test_dataset = (\n",
    "          (raw_test_dataset, transform_fn) | tft_beam.TransformDataset())\n",
    "      # Don't need transformed data schema, it's the same as before.\n",
    "      transformed_test_data, _ = transformed_test_dataset\n",
    "\n",
    "      _ = (\n",
    "          transformed_test_data\n",
    "          | 'EncodeTestData' >> beam.Map(transformed_data_coder.encode)\n",
    "          | 'WriteTestData' >> beam.io.WriteToTFRecord(\n",
    "              os.path.join(working_dir, TRANSFORMED_TEST_DATA_FILEBASE)))\n",
    "\n",
    "      # Will write a SavedModel and metadata to working_dir, which can then\n",
    "      # be read by the tft.TFTransformOutput class.\n",
    "      _ = (\n",
    "          transform_fn\n",
    "          | 'WriteTransformFn' >> tft_beam.WriteTransformFn(working_dir))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After we have preprocessed our data, we're ready to ingest the transformed data for training. We will create our `input_fn` which will return a `tf.data` pipeline. Note that we are using the `transformed_feature_spec` function applied to our `tf.transform` output to build in the metadata from TF Transform into our `tf.data` pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def input_fn(tf_transform_output, transformed_examples_pattern, batch_size):\n",
    "  \"\"\"An input function reading from transformed data, converting to model input.\n",
    "\n",
    "  Args:\n",
    "    tf_transform_output: Wrapper around output of tf.Transform.\n",
    "    transformed_examples_pattern: Base filename of examples.\n",
    "    batch_size: Batch size.\n",
    "\n",
    "  Returns:\n",
    "    The input data for training or eval, in the form of k.\n",
    "  \"\"\"\n",
    "  return tf.data.experimental.make_batched_features_dataset(\n",
    "      file_pattern=transformed_examples_pattern,\n",
    "      batch_size=batch_size,\n",
    "      features=tf_transform_output.transformed_feature_spec(),\n",
    "      reader=tf.data.TFRecordDataset,\n",
    "      label_key=LABEL_KEY,\n",
    "      shuffle=True).prefetch(tf.data.experimental.AUTOTUNE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But, what if the input data has not already been transformed? We don't want to rerun the entire pipeline necessarily, but rather apply the transformations from our TF Transform pipeline to the batches of data as they're being passed to the model. Here we will leverage a `transform_features_layer` to preprocess the features (using the metadata and TF graph generated by TF Transform) before passing data into the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def input_fn_raw(tf_transform_output, raw_examples_pattern, batch_size):\n",
    "  \"\"\"An input function reading from raw data, converting to model input.\n",
    "\n",
    "  Args:\n",
    "    tf_transform_output: Wrapper around output of tf.Transform.\n",
    "    raw_examples_pattern: Base filename of examples.\n",
    "    batch_size: Batch size.\n",
    "\n",
    "  Returns:\n",
    "    The input data for training or eval, in the form of k.\n",
    "  \"\"\"\n",
    " \n",
    "  # Order incoming features in expected order from the list defined earlier.\n",
    "  def get_ordered_raw_data_dtypes():\n",
    "    result = []\n",
    "    for col in ORDERED_CSV_COLUMNS:\n",
    "      if col not in RAW_DATA_FEATURE_SPEC:\n",
    "        result.append(0.0)\n",
    "        continue\n",
    "      spec = RAW_DATA_FEATURE_SPEC[col]\n",
    "      if isinstance(spec, tf.io.FixedLenFeature):\n",
    "        result.append(spec.dtype)\n",
    "      else:\n",
    "        result.append(0.0)\n",
    "    return result\n",
    "\n",
    "  dataset = tf.data.experimental.make_csv_dataset(\n",
    "      file_pattern=raw_examples_pattern,\n",
    "      batch_size=batch_size,\n",
    "      column_names=ORDERED_CSV_COLUMNS,\n",
    "      column_defaults=get_ordered_raw_data_dtypes(),\n",
    "      prefetch_buffer_size=0,\n",
    "      ignore_errors=True)\n",
    "\n",
    "  tft_layer = tf_transform_output.transform_features_layer()\n",
    "\n",
    "  def transform_dataset(data):\n",
    "    raw_features = {}\n",
    "    for key, val in data.items():\n",
    "      if key not in RAW_DATA_FEATURE_SPEC:\n",
    "        continue\n",
    "      if isinstance(RAW_DATA_FEATURE_SPEC[key], tf.io.VarLenFeature):\n",
    "        raw_features[key] = tf.RaggedTensor.from_tensor(\n",
    "            tf.expand_dims(val, -1)).to_sparse()\n",
    "        continue\n",
    "      raw_features[key] = val\n",
    "    transformed_features = tft_layer(raw_features)\n",
    "    data_labels = transformed_features.pop(LABEL_KEY)\n",
    "    return (transformed_features, data_labels)\n",
    "\n",
    "  return dataset.map(\n",
    "      transform_dataset,\n",
    "      num_parallel_calls=tf.data.experimental.AUTOTUNE).prefetch(\n",
    "          tf.data.experimental.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our training and test datasets have now been transformed! However, we need to ensure that we avoid training-serving skew by applying the transformations at prediction time as well. To ensure this, we need to create a function that includes our `transform_fn` as part of the model. We will do this by creating a new layer in our model via the `transform_feature_layer` function. We define a function `serve_tf_examples_fn` and decorate it as a `tf.function` so that it can be compiled into the model graph. Finally we will create a concrete function via `get_concrete_function` so we can include it in the model signatures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_serving_model(tf_transform_output, model, output_dir):\n",
    "  \"\"\"Exports a keras model for serving.\n",
    "\n",
    "  Args:\n",
    "    tf_transform_output: Wrapper around output of tf.Transform.\n",
    "    model: A keras model to export for serving.\n",
    "    output_dir: A directory where the model will be exported to.\n",
    "  \"\"\"\n",
    "  # The layer has to be saved to the model for keras tracking purpases.\n",
    "  model.tft_layer = tf_transform_output.transform_features_layer()\n",
    "\n",
    "  @tf.function\n",
    "  def serve_tf_examples_fn(serialized_tf_examples):\n",
    "    \"\"\"Serving tf.function model wrapper.\"\"\"\n",
    "    feature_spec = RAW_DATA_FEATURE_SPEC.copy()\n",
    "    feature_spec.pop(LABEL_KEY)\n",
    "    parsed_features = tf.io.parse_example(serialized_tf_examples, feature_spec)\n",
    "    transformed_features = model.tft_layer(parsed_features)\n",
    "    outputs = model(transformed_features)\n",
    "    classes_names = tf.constant([['0', '1']])\n",
    "    classes = tf.tile(classes_names, [tf.shape(outputs)[0], 1])\n",
    "    return {'classes': classes, 'scores': outputs}\n",
    "\n",
    "  concrete_serving_fn = serve_tf_examples_fn.get_concrete_function(\n",
    "      tf.TensorSpec(shape=[None], dtype=tf.string, name='inputs'))\n",
    "  signatures = {'serving_default': concrete_serving_fn}\n",
    "\n",
    "  # This is required in order to make this model servable with model_server.\n",
    "  versioned_output_dir = os.path.join(output_dir, '1')\n",
    "  model.save(versioned_output_dir, save_format='tf', signatures=signatures)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we're ready to define our training and evaluation loop! For the most part, this is the same standard function we've seen in previous examples. The biggest difference is that we include the output directory of the TF Transform pipeline so that we can transform the data as needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate(raw_train_eval_data_path_pattern,\n",
    "                       transformed_train_eval_data_path_pattern,\n",
    "                       output_dir,\n",
    "                       transform_output_dir,\n",
    "                       num_train_instances=NUM_TRAIN_INSTANCES,\n",
    "                       num_test_instances=NUM_TEST_INSTANCES):\n",
    "  \"\"\"Train the model on training data and evaluate on test data.\n",
    "\n",
    "  Args:\n",
    "    raw_train_eval_data_path_pattern: A pair of patterns of raw\n",
    "      (train data file paths, eval data file paths) in CSV format.\n",
    "    transformed_train_eval_data_path_pattern: A pair of patterns of transformed\n",
    "      (train data file paths, eval data file paths) in TFRecord format.\n",
    "    output_dir: A directory where the output should be exported to.\n",
    "    transform_output_dir: The location of the Transform output.\n",
    "    num_train_instances: Number of instances in train set\n",
    "    num_test_instances: Number of instances in test set\n",
    "\n",
    "  Returns:\n",
    "    The results from the estimator's 'evaluate' method\n",
    "  \"\"\"\n",
    "  if not ((raw_train_eval_data_path_pattern is None) ^\n",
    "          (transformed_train_eval_data_path_pattern is None)):\n",
    "    raise ValueError(\n",
    "        'Exactly one of raw_train_eval_data_path_pattern and '\n",
    "        'transformed_train_eval_data_path_pattern should be provided')\n",
    "  tf_transform_output = tft.TFTransformOutput(transform_output_dir)\n",
    "\n",
    "  if raw_train_eval_data_path_pattern is not None:\n",
    "    selected_input_fn = input_fn_raw\n",
    "    (train_data_path_pattern,\n",
    "     eval_data_path_pattern) = raw_train_eval_data_path_pattern\n",
    "  else:\n",
    "    selected_input_fn = input_fn\n",
    "    (train_data_path_pattern,\n",
    "     eval_data_path_pattern) = transformed_train_eval_data_path_pattern\n",
    "\n",
    "  train_dataset = selected_input_fn(\n",
    "      tf_transform_output, train_data_path_pattern, batch_size=TRAIN_BATCH_SIZE)\n",
    "\n",
    "  # Evaluate model on test dataset.\n",
    "  validation_dataset = selected_input_fn(\n",
    "      tf_transform_output, eval_data_path_pattern, batch_size=TRAIN_BATCH_SIZE)\n",
    "\n",
    "  feature_spec = tf_transform_output.transformed_feature_spec().copy()\n",
    "  feature_spec.pop(LABEL_KEY)\n",
    "\n",
    "  inputs = {}\n",
    "  for key, spec in feature_spec.items():\n",
    "    if isinstance(spec, tf.io.VarLenFeature):\n",
    "      inputs[key] = tf.keras.layers.Input(\n",
    "          shape=[None], name=key, dtype=spec.dtype, sparse=True)\n",
    "    elif isinstance(spec, tf.io.FixedLenFeature):\n",
    "      inputs[key] = tf.keras.layers.Input(\n",
    "          shape=spec.shape, name=key, dtype=spec.dtype)\n",
    "    else:\n",
    "      raise ValueError('Spec type is not supported: ', key, spec)\n",
    "\n",
    "  encoded_inputs = {}\n",
    "  for key in inputs:\n",
    "    feature = tf.expand_dims(inputs[key], -1)\n",
    "    if key in CATEGORICAL_FEATURE_KEYS:\n",
    "      num_buckets = tf_transform_output.num_buckets_for_transformed_feature(key)\n",
    "      encoding_layer = (\n",
    "          tf.keras.layers.experimental.preprocessing.CategoryEncoding(\n",
    "              max_tokens=num_buckets, output_mode='binary', sparse=False))\n",
    "      encoded_inputs[key] = encoding_layer(feature)\n",
    "    else:\n",
    "      encoded_inputs[key] = feature\n",
    "\n",
    "  stacked_inputs = tf.concat(tf.nest.flatten(encoded_inputs), axis=1)\n",
    "  output = tf.keras.layers.Dense(100, activation='relu')(stacked_inputs)\n",
    "  output = tf.keras.layers.Dense(70, activation='relu')(output)\n",
    "  output = tf.keras.layers.Dense(50, activation='relu')(output)\n",
    "  output = tf.keras.layers.Dense(20, activation='relu')(output)\n",
    "  output = tf.keras.layers.Dense(2, activation='sigmoid')(output)\n",
    "  model = tf.keras.Model(inputs=inputs, outputs=output)\n",
    "\n",
    "  model.compile(optimizer='adam',\n",
    "                loss='binary_crossentropy',\n",
    "                metrics=['accuracy'])\n",
    "  logging.info(model.summary())\n",
    "\n",
    "  model.fit(train_dataset, validation_data=validation_dataset,\n",
    "            epochs=TRAIN_NUM_EPOCHS,\n",
    "            verbose=2,\n",
    "            steps_per_epoch=math.ceil(num_train_instances / TRAIN_BATCH_SIZE),\n",
    "            validation_steps=math.ceil(num_test_instances / TRAIN_BATCH_SIZE))\n",
    "\n",
    "  # Export the model.\n",
    "  export_serving_model(tf_transform_output, model, output_dir)\n",
    "\n",
    "  return model.evaluate(validation_dataset, steps=num_test_instances)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(input_data_dir,\n",
    "         working_dir,\n",
    "         read_raw_data_for_training=True,\n",
    "         num_train_instances=NUM_TRAIN_INSTANCES,\n",
    "         num_test_instances=NUM_TEST_INSTANCES):\n",
    "  if not working_dir:\n",
    "    working_dir = tempfile.mkdtemp(dir=input_data_dir)\n",
    "\n",
    "  train_data_file = os.path.join(input_data_dir, 'adult.data')\n",
    "  test_data_file = os.path.join(input_data_dir, 'adult.test')\n",
    "\n",
    "  transform_data(train_data_file, test_data_file, working_dir)\n",
    "\n",
    "  if read_raw_data_for_training:\n",
    "    raw_train_and_eval_patterns = (train_data_file, test_data_file)\n",
    "    transformed_train_and_eval_patterns = None\n",
    "  else:\n",
    "    train_pattern = os.path.join(working_dir,\n",
    "                                 TRANSFORMED_TRAIN_DATA_FILEBASE + '*')\n",
    "    eval_pattern = os.path.join(working_dir,\n",
    "                                TRANSFORMED_TEST_DATA_FILEBASE + '*')\n",
    "    raw_train_and_eval_patterns = None\n",
    "    transformed_train_and_eval_patterns = (train_pattern, eval_pattern)\n",
    "  output_dir = os.path.join(working_dir, EXPORTED_MODEL_DIR)\n",
    "  results = train_and_evaluate(\n",
    "      raw_train_and_eval_patterns,\n",
    "      transformed_train_and_eval_patterns,\n",
    "      output_dir,\n",
    "      working_dir,\n",
    "      num_train_instances=num_train_instances,\n",
    "      num_test_instances=num_test_instances)\n",
    "\n",
    "  pprint.pprint(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're ready to train our model. We will run our main function defined above to execute the training loop after providing the input directory containing our data and the working directory we wish to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_DATA_DIR=\"./input_data\"\n",
    "WORKING_DIR=\"./working_dir\"\n",
    "\n",
    "main(INPUT_DATA_DIR, WORKING_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have saved a trained model with our custom serving input function to apply our tranformed defined by TF Transform. Let's now be sure that everything works!\n",
    "\n",
    "To see a little bit of information about what inputs are expected, we can use the `saved_model_cli show` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!saved_model_cli show --dir ./working_dir/exported_model_dir/1/ --tag_set serve --signature_def serving_default"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that we expect a scalar input of a string for our prediction. Why is this? Well recall that we created a pipeline to import TFRecords as `tf.Example`s. We need to pass along the tensor in the protobuf format to match what we saw at training time. Fortunately, the `text_format` function in the `google-protobuf` package can handle this for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.protobuf import text_format\n",
    "\n",
    "_PREDICT_TF_EXAMPLE_TEXT_PB = \"\"\"\n",
    "    features {\n",
    "      feature {\n",
    "        key: \"age\"\n",
    "        value { float_list: { value: 25 } }\n",
    "      }\n",
    "      feature {\n",
    "        key: \"workclass\"\n",
    "        value { bytes_list: { value: \"Private\" } }\n",
    "      }\n",
    "      feature {\n",
    "        key: \"education\"\n",
    "        value { bytes_list: { value: \"11th\" } }\n",
    "      }\n",
    "      feature {\n",
    "        key: \"education-num\"\n",
    "        value { float_list: { value: 7 } }\n",
    "      }\n",
    "      feature {\n",
    "        key: \"marital-status\"\n",
    "        value { bytes_list: { value: \"Never-married\" } }\n",
    "      }\n",
    "      feature {\n",
    "        key: \"occupation\"\n",
    "        value { bytes_list: { value: \"Machine-op-inspct\" } }\n",
    "      }\n",
    "      feature {\n",
    "        key: \"relationship\"\n",
    "        value { bytes_list: { value: \"Own-child\" } }\n",
    "      }\n",
    "      feature {\n",
    "        key: \"race\"\n",
    "        value { bytes_list: { value: \"Black\" } }\n",
    "      }\n",
    "      feature {\n",
    "        key: \"sex\"\n",
    "        value { bytes_list: { value: \"Male\" } }\n",
    "      }\n",
    "      feature {\n",
    "        key: \"capital-gain\"\n",
    "        value { float_list: { value: 0 } }\n",
    "      }\n",
    "      feature {\n",
    "        key: \"capital-loss\"\n",
    "        value { float_list: { value: 0 } }\n",
    "      }\n",
    "      feature {\n",
    "        key: \"hours-per-week\"\n",
    "        value { float_list: { value: 40 } }\n",
    "      }\n",
    "      feature {\n",
    "        key: \"native-country\"\n",
    "        value { bytes_list: { value: \"United-States\" } }\n",
    "      }\n",
    "    }\n",
    "    \"\"\"\n",
    "\n",
    "_MODEL_NAME = 'my_model'\n",
    "\n",
    "_CLASSIFICATION_REQUEST_TEXT_PB = \"\"\"model_spec { name: \"%s\" }\n",
    "    input {\n",
    "      example_list {\n",
    "        examples {\n",
    "          %s\n",
    "        }\n",
    "      }\n",
    "    }\"\"\" % (_MODEL_NAME, _PREDICT_TF_EXAMPLE_TEXT_PB)\n",
    "\n",
    "\n",
    "model = tf.keras.models.load_model('./working_dir/exported_model_dir/1/')\n",
    "\n",
    "example = text_format.Parse(_PREDICT_TF_EXAMPLE_TEXT_PB, tf.train.Example())\n",
    "prediction = model.signatures['serving_default'](\n",
    "    tf.constant([example.SerializeToString()], tf.string))\n",
    "\n",
    "print(prediction['scores'].numpy())"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "name": "tf2-2-3-gpu.2-3.m58",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-2-3-gpu.2-3:m58"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
