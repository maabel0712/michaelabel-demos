{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "environment": {
      "name": "tf2-gpu.2-1.m48",
      "type": "gcloud",
      "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-1:m48"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "colab": {
      "name": "Multi-GPU Training on CAIP [Shared]",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N22NDjIjjxcI"
      },
      "source": [
        "# Distributed Training with GPUs on Cloud AI Platform\n",
        "\n",
        "In this notebook we will walk through using Cloud AI Platform to perform distributed training using the `MirroredStrategy` found within `tf.keras`. This strategy will allow us to use the synchronous AllReduce strategy on a VM with multiple GPUs attached.\n",
        "\n",
        "First we need to authenicate our Google Cloud account to be able to submit training jobs to AI Platform. We will do this via `gcloud auth login` to authenicate with OAuth2."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nFUhd2ZIBNK0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5b62b10b-d5fd-4c5f-93f8-22d6104d1056"
      },
      "source": [
        "# authorize your gcp account\n",
        "!gcloud auth login"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to the following link in your browser:\n",
            "\n",
            "    https://accounts.google.com/o/oauth2/auth?response_type=code&client_id=32555940559.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=openid+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fuserinfo.email+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fcloud-platform+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fappengine.admin+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fcompute+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Faccounts.reauth&state=hxv6UaFkCKRtQge51Cxt8IL9Gx8yOK&prompt=consent&access_type=offline&code_challenge=U9JdSfqDOPMFWiWdEX9YKsukCEmmh8HxYMFNawQyNGI&code_challenge_method=S256\n",
            "\n",
            "Enter verification code: 4/1AY0e-g5z7sAlNXwXHiVZd5od8pfvvb1lEuIu4Q7_XF4O9k27zzAviW9yyhk\n",
            "\n",
            "You are now logged in as [michaelabel.gcp@gmail.com].\n",
            "Your current project is [michaelabel-gcp-training].  You can change this setting by running:\n",
            "  $ gcloud config set project PROJECT_ID\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "63e-3SdLlh7K"
      },
      "source": [
        "Next we will configure our environment. Be sure to change the `PROJECT_ID` variable in the below cell to your Project ID. This will be the project to which the Cloud AI Platform resources will be billed. We will also create a bucket for our training artifacts (if it does not already exist)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TKZYJbBkBcOk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d7196ec2-0ba2-4b44-c8fa-9926ce73c20a"
      },
      "source": [
        "# config\n",
        "import os\n",
        "PROJECT_ID = 'michaelabel-gcp-training'  # TODO: Insert your Project ID here!\n",
        "BUCKET = PROJECT_ID + '-accelerator-demo'\n",
        "REGION = 'us-central1'\n",
        "os.environ[\"PROJECT_ID\"] = PROJECT_ID\n",
        "os.environ[\"BUCKET\"] = BUCKET\n",
        "\n",
        "!gcloud config set project {PROJECT_ID}\n",
        "!gsutil mb gs://{BUCKET}\n",
        "!gcloud config set compute/region {REGION}"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Updated property [core/project].\n",
            "Creating gs://michaelabel-gcp-training-accelerator-demo/...\n",
            "ServiceException: 409 Bucket michaelabel-gcp-training-accelerator-demo already exists.\n",
            "Updated property [compute/region].\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jth9W8NtmNUD"
      },
      "source": [
        "Since we are going to submit our training job to Cloud AI Platform, we need to create our trainer package. We will create the `train` directory for our package and create a blank `__init__.py` file so Python knows that this folder contains a package."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cavM78bf_mU4"
      },
      "source": [
        "!mkdir train\n",
        "!touch train/__init__.py"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PIMQo_lImhn_"
      },
      "source": [
        "Next we will create a module containing a function which will create our model. Note that we will be using the Fashion MNIST dataset. Since it's a small dataset, we will simply load it into memory for getting the parameters for our model.\n",
        "\n",
        "Our model will be a DNN with only dense layers, applying dropout to each hidden layer. We will also use ReLU activation for all hidden layers."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "88-9WeCQ_mU9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "56ff944b-7069-4850-9da7-fbae4fadccb2"
      },
      "source": [
        "%%writefile train/model_definition.py\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "# Get data\n",
        "\n",
        "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.fashion_mnist.load_data()\n",
        "\n",
        "# add empty color dimension\n",
        "x_train = np.expand_dims(x_train, -1)\n",
        "x_test = np.expand_dims(x_test, -1)\n",
        "\n",
        "def create_model():\n",
        "    model = tf.keras.models.Sequential()\n",
        "    model.add(tf.keras.layers.Flatten(input_shape=x_train.shape[1:]))\n",
        "    model.add(tf.keras.layers.Dense(1028))\n",
        "    model.add(tf.keras.layers.Activation('relu'))\n",
        "    model.add(tf.keras.layers.Dropout(0.5))\n",
        "    model.add(tf.keras.layers.Dense(512))\n",
        "    model.add(tf.keras.layers.Activation('relu'))\n",
        "    model.add(tf.keras.layers.Dropout(0.5))\n",
        "    model.add(tf.keras.layers.Dense(256))\n",
        "    model.add(tf.keras.layers.Activation('relu'))\n",
        "    model.add(tf.keras.layers.Dropout(0.5))\n",
        "    model.add(tf.keras.layers.Dense(10))\n",
        "    model.add(tf.keras.layers.Activation('softmax'))\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Writing train/model_definition.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0DOKsnDhnU87"
      },
      "source": [
        "Before we submit our training jobs to Cloud AI Platform, let's be sure our model runs locally. We will call the `model_definition` function to create our model and use `tf.keras.datasets.fashion_mnist.load_data()` to import the Fashion MNIST dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r8bPcX7T-SH1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b365f40f-3f25-40bb-8cd1-0cee4d85a8a7"
      },
      "source": [
        "import os\n",
        "import os\n",
        "import time\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from train import model_definition\n",
        "\n",
        "#Get data\n",
        "\n",
        "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.fashion_mnist.load_data()\n",
        "\n",
        "# add empty color dimension\n",
        "x_train = np.expand_dims(x_train, -1)\n",
        "x_test = np.expand_dims(x_test, -1)\n",
        "\n",
        "def create_dataset(X, Y, epochs, batch_size):\n",
        "    dataset = tf.data.Dataset.from_tensor_slices((X, Y))\n",
        "    dataset = dataset.repeat(epochs).batch(batch_size, drop_remainder=True)\n",
        "    return dataset\n",
        "\n",
        "ds_train = create_dataset(x_train, y_train, 1, 5000)\n",
        "ds_test = create_dataset(x_test, y_test, 1, 1000)\n",
        "\n",
        "model = model_definition.create_model()\n",
        "\n",
        "model.compile(\n",
        "  optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3, ),\n",
        "  loss='sparse_categorical_crossentropy',\n",
        "  metrics=['sparse_categorical_accuracy'])\n",
        "    \n",
        "start = time.time()\n",
        "\n",
        "model.fit(\n",
        "    ds_train,\n",
        "    validation_data=ds_test, \n",
        "    verbose=1\n",
        ")\n",
        "print(\"Training time without GPUs locally: {}\".format(time.time() - start))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz\n",
            "32768/29515 [=================================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz\n",
            "26427392/26421880 [==============================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz\n",
            "8192/5148 [===============================================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz\n",
            "4423680/4422102 [==============================] - 0s 0us/step\n",
            "12/12 [==============================] - 9s 729ms/step - loss: 75.6025 - sparse_categorical_accuracy: 0.3110 - val_loss: 1.8982 - val_sparse_categorical_accuracy: 0.6896\n",
            "Training time without GPUs locally: 10.302881956100464\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L_U-u_tZ_mVK"
      },
      "source": [
        "\n",
        "\n",
        "# Train on multiple GPUs/CPUs with MultiWorkerMirrored Strategy\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P0VQ7GUsn8wb"
      },
      "source": [
        "That took a few minutes to train our model for 20 epochs. Let's see how we can do better using Cloud AI Platform. We will be leveraging the `MultiWorkerMirroredStrategy` supplied in `tf.distribute`. The main difference between this code, and the code from the local test is that we need to compile the model within the scope of the strategy. When we do this our training op will use information stored in the `TF_CONFIG` variable to assign ops to the various devices for the AllReduce strategy. \n",
        "\n",
        "After the training process finishes, we will print out the time spent training. Since it takes a few minutes to spin up the resources being used for training on Cloud AI Platform, and this time can vary, we want a consistent measure of how long training took.\n",
        "\n",
        "Note: When we train models on Cloud AI Platform, the `TF_CONFIG` variable is automatically set. So we do not need to worry about adjusting based what cluster configuration we use."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_AF4VDhT_mVg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "743d50df-a135-42af-9330-61c0828b22f3"
      },
      "source": [
        "%%writefile train/train_mult_worker_mirrored.py\n",
        "import os\n",
        "import os\n",
        "import time\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from . import model_definition\n",
        "\n",
        "strategy = tf.distribute.experimental.MultiWorkerMirroredStrategy()\n",
        "\n",
        "#Get data\n",
        "\n",
        "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.fashion_mnist.load_data()\n",
        "\n",
        "# add empty color dimension\n",
        "x_train = np.expand_dims(x_train, -1)\n",
        "x_test = np.expand_dims(x_test, -1)\n",
        "\n",
        "def create_dataset(X, Y, epochs, batch_size):\n",
        "    dataset = tf.data.Dataset.from_tensor_slices((X, Y))\n",
        "    dataset = dataset.repeat(epochs).batch(batch_size, drop_remainder=True)\n",
        "    return dataset\n",
        "\n",
        "ds_train = create_dataset(x_train, y_train, 20, 5000)\n",
        "ds_test = create_dataset(x_test, y_test, 1, 1000)\n",
        "\n",
        "print('Number of devices: {}'.format(strategy.num_replicas_in_sync))\n",
        "\n",
        "with strategy.scope():\n",
        "    model = model_definition.create_model()\n",
        "    model.compile(\n",
        "      optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3, ),\n",
        "      loss='sparse_categorical_crossentropy',\n",
        "      metrics=['sparse_categorical_accuracy'])\n",
        "    \n",
        "start = time.time()\n",
        "\n",
        "model.fit(\n",
        "    ds_train,\n",
        "    validation_data=ds_test, \n",
        "    verbose=2\n",
        ")\n",
        "print(\"Training time with multiple GPUs: {}\".format(time.time() - start))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Writing train/train_mult_worker_mirrored.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ViUZcz7Tp9Kp"
      },
      "source": [
        "First we will train a model without using GPUs to give us a baseline. We will use a consistent format throughout the trials. We will define a `config.yaml` file to contain our cluster configuration and the pass this file in as the value of a command-line argument `--config`.\n",
        "\n",
        "In our first example, we will use a single `n1-highcpu-16` VM."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sBJw5hJAadVW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e500ee0b-d7a1-43f5-98bf-9081f43b3c16"
      },
      "source": [
        "%%writefile config.yaml\n",
        "trainingInput:\n",
        "  scaleTier: CUSTOM\n",
        "  masterType: n1-highcpu-16"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Writing config.yaml\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_mlylgvCaeXW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dd1df584-6868-4359-af54-9804dde60f8b"
      },
      "source": [
        "%%bash\n",
        "\n",
        "now=$(date +\"%Y%m%d_%H%M%S\")\n",
        "JOB_NAME=\"cpu_only_fashion_minst_$now\"\n",
        "\n",
        "gcloud ai-platform jobs submit training $JOB_NAME \\\n",
        "  --staging-bucket=gs://$BUCKET \\\n",
        "  --package-path=train \\\n",
        "  --module-name=train.train_mult_worker_mirrored \\\n",
        "  --runtime-version=2.1 \\\n",
        "  --python-version=3.7 \\\n",
        "  --region=us-west1 \\\n",
        "  --config config.yaml"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "jobId: cpu_only_fashion_minst_20201207_140943\n",
            "state: QUEUED\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Job [cpu_only_fashion_minst_20201207_140943] submitted successfully.\n",
            "Your job is still active. You may view the status of your job with the command\n",
            "\n",
            "  $ gcloud ai-platform jobs describe cpu_only_fashion_minst_20201207_140943\n",
            "\n",
            "or continue streaming the logs with the command\n",
            "\n",
            "  $ gcloud ai-platform jobs stream-logs cpu_only_fashion_minst_20201207_140943\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g9tji3XRqbi-"
      },
      "source": [
        "If we go through the logs, we see that the training took around 30 seconds (though the exact number may vary for your case). Let's now attach two Nvidia Tesla K80 GPUs and rerun the training job."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fCGWARBH_mVi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "75ee1fce-8115-40fa-f13e-a3519907e30f"
      },
      "source": [
        "%%writefile config.yaml\n",
        "trainingInput:\n",
        "  scaleTier: CUSTOM\n",
        "  masterType: n1-highcpu-16\n",
        "  masterConfig:\n",
        "    acceleratorConfig:\n",
        "      count: 2\n",
        "      type: NVIDIA_TESLA_K80"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Overwriting config.yaml\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UmXeeg6r_mVk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "79162ff7-0a3f-4cf3-863e-0b96a86e5a0d"
      },
      "source": [
        "%%bash\n",
        "\n",
        "now=$(date +\"%Y%m%d_%H%M%S\")\n",
        "JOB_NAME=\"multi_gpu_fashion_minst_2gpu_$now\"\n",
        "\n",
        "gcloud ai-platform jobs submit training $JOB_NAME \\\n",
        "  --staging-bucket=gs://$BUCKET \\\n",
        "  --package-path=train \\\n",
        "  --module-name=train.train_mult_worker_mirrored \\\n",
        "  --runtime-version=2.1 \\\n",
        "  --python-version=3.7 \\\n",
        "  --region=us-west1 \\\n",
        "  --config config.yaml"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "jobId: multi_gpu_fashion_minst_2gpu_20201207_140946\n",
            "state: QUEUED\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Job [multi_gpu_fashion_minst_2gpu_20201207_140946] submitted successfully.\n",
            "Your job is still active. You may view the status of your job with the command\n",
            "\n",
            "  $ gcloud ai-platform jobs describe multi_gpu_fashion_minst_2gpu_20201207_140946\n",
            "\n",
            "or continue streaming the logs with the command\n",
            "\n",
            "  $ gcloud ai-platform jobs stream-logs multi_gpu_fashion_minst_2gpu_20201207_140946\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0fMLxLOgq7mc"
      },
      "source": [
        "That was a lot faster! On my end it took around 6 seconds to train the model. Let's keep going and add more GPUs!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ocXYk61hGRG_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ef840694-ef5d-4e62-cd88-4ad8b3f3c664"
      },
      "source": [
        "%%writefile config.yaml\n",
        "trainingInput:\n",
        "  scaleTier: CUSTOM\n",
        "  masterType: n1-highcpu-16\n",
        "  masterConfig:\n",
        "    acceleratorConfig:\n",
        "      count: 4\n",
        "      type: NVIDIA_TESLA_K80"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Overwriting config.yaml\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MKRRVDLhZoj3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "59f93e27-4806-46d5-b4a6-e678486ee2a9"
      },
      "source": [
        "%%bash\n",
        "\n",
        "now=$(date +\"%Y%m%d_%H%M%S\")\n",
        "JOB_NAME=\"multi_gpu_fashion_minst_4gpu_$now\"\n",
        "\n",
        "gcloud ai-platform jobs submit training $JOB_NAME \\\n",
        "  --staging-bucket=gs://$BUCKET \\\n",
        "  --package-path=train \\\n",
        "  --module-name=train.train_mult_worker_mirrored \\\n",
        "  --runtime-version=2.1 \\\n",
        "  --python-version=3.7 \\\n",
        "  --region=us-west1 \\\n",
        "  --config config.yaml"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "jobId: multi_gpu_fashion_minst_4gpu_20201207_140948\n",
            "state: QUEUED\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Job [multi_gpu_fashion_minst_4gpu_20201207_140948] submitted successfully.\n",
            "Your job is still active. You may view the status of your job with the command\n",
            "\n",
            "  $ gcloud ai-platform jobs describe multi_gpu_fashion_minst_4gpu_20201207_140948\n",
            "\n",
            "or continue streaming the logs with the command\n",
            "\n",
            "  $ gcloud ai-platform jobs stream-logs multi_gpu_fashion_minst_4gpu_20201207_140948\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F3PfreD5rfgE"
      },
      "source": [
        "Wait...what happened? It took around 10 seconds to train the model in this case. It was faster than no GPUs, but why was it slower than 2 GPUs? If you rerun this job with 8 GPUs you'll actually see it takes just as long as using no GPUs!\n",
        "\n",
        "The answer is in our input pipeline. In short, the I/O involved in using more GPUs started to outweigh the benefits of having more available devices. We can try to improve our input pipelines to overcome this (e.g. using caching, adjusting batch size, etc.). \n",
        "\n",
        "Finally, what would the `config.yaml` look like if we wanted to use multiple VMs as well? We include that example here, but will not run the training job. Note that we don't have to change any code in our trainer package! However, we may want to consider our input pipeline to really take advantage of the larger cluster."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UAZqx2h8dKzq"
      },
      "source": [
        "%%writefile config.yaml\n",
        "trainingInput:\n",
        "  scaleTier: CUSTOM\n",
        "  # Configure a master worker with 4 T4 GPUs\n",
        "  masterType: n1-highcpu-16\n",
        "  masterConfig:\n",
        "    acceleratorConfig:\n",
        "      count: 4\n",
        "      type: NVIDIA_TESLA_T4\n",
        "  # Configure 9 workers, each with 4 T4 GPUs\n",
        "  workerCount: 9\n",
        "  workerType: n1-highcpu-16\n",
        "  workerConfig:\n",
        "    acceleratorConfig:\n",
        "      count: 4\n",
        "      type: NVIDIA_TESLA_T4"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}